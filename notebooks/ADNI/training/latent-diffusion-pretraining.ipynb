{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "sys.path.append('../../lib/src/')\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import lightning as L\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from lib.src.pythae.models import VAE\n",
    "from lib.scripts.utils import Encoder_Chairs,Decoder_Chairs, My_Dataset, My_MaskedDataset, make_batched_masks\n",
    "from lib.src.pythae.models.vae import VAEConfig\n",
    "from lib.src.pythae.trainers import BaseTrainerConfig\n",
    "from lib.src.pythae.pipelines.training import TrainingPipeline\n",
    "from lib.src.pythae.samplers.normal_sampling import NormalSampler\n",
    "from lib.src.pythae.samplers.manifold_sampler import RHVAESampler\n",
    "\n",
    "sys.path.append('diffusion/stable_diffusion/')\n",
    "sys.path.append('diffusion/stable_diffusion/model/')\n",
    "from diffusion.stable_diffusion.latent_diffusion import LatentDiffusion, DiffusionWrapper, MyLatentDiffusion, LitLDM\n",
    "from diffusion.stable_diffusion.model.autoencoder import Autoencoder\n",
    "from diffusion.stable_diffusion.model.clip_embedder import CLIPTextEmbedder\n",
    "from diffusion.stable_diffusion.model.unet import UNetModel, _test_time_embeddings\n",
    "from diffusion.stable_diffusion.sampler.ddim import DDIMSampler\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8000, 8, 120])\n",
      "torch.Size([3200, 120])\n"
     ]
    }
   ],
   "source": [
    "train_data = torch.load('data-models/adni/data/ADNI_train.pt') #(N, T, D)\n",
    "eval_data = torch.load('data-models/adni/data/ADNI_eval.pt')\n",
    "test_data = torch.load('data-models/adni/data/ADNI_test.pt')\n",
    "print(train_data.shape)\n",
    "\n",
    "train_seq_mask = torch.load('data-models/adni/data/ADNI_train_seq_mask.pt') #(N, T)\n",
    "eval_seq_mask = torch.load('data-models/adni/data/ADNI_eval_seq_mask.pt')\n",
    "test_seq_mask = torch.load('data-models/adni/data/ADNI_test_seq_mask.pt')\n",
    "\n",
    "#Keeping only last observations when available\n",
    "train_data = train_data[train_seq_mask[:, -1] == 1, -1] \n",
    "eval_data = eval_data[eval_seq_mask[:, -1] == 1, -1]\n",
    "test_data = test_data[test_seq_mask[:, -1] == 1, -1]\n",
    "\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent dim: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (decoder): Decoder_ADNI(\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=9, out_features=15, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=15, out_features=30, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=30, out_features=60, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=60, out_features=120, bias=True)\n",
       "      (7): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (encoder): Encoder_ADNI(\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=120, out_features=60, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=60, out_features=30, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=30, out_features=15, bias=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (embedding): Linear(in_features=15, out_features=9, bias=True)\n",
       "    (log_var): Linear(in_features=15, out_features=9, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = (1, 120)\n",
    "f = 40 #subsampling factor\n",
    "latent_dim = 1* (input_dim[1] // f) * (input_dim[1] // f)\n",
    "print('Latent dim:', latent_dim)\n",
    "\n",
    "device = 'cuda'\n",
    "vae = VAE.load_from_folder('data-models/adni/trained_models/pre-trained_vae/final_model').to(device)\n",
    "vae.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input U-NET:  torch.Size([128, 1, 3, 3])\n",
      "Shape of output:  torch.Size([128, 1, 3, 3])\n",
      "Number of trainable params:  353953\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_config_unet(config):\n",
    "    return UNetModel(\n",
    "        in_channels=config['in_channels'],\n",
    "        out_channels=config['out_channels'],\n",
    "        channels=config['channels'],\n",
    "        n_res_blocks=config['n_res_blocks'],\n",
    "        attention_levels=config['attention_levels'],\n",
    "        channel_multipliers=config['channel_multipliers'],\n",
    "        n_heads=config['n_heads'],\n",
    "    )\n",
    "\n",
    "in_channels = 1\n",
    "out_channels = 1\n",
    "channels = 32\n",
    "n_res_blocks = 2\n",
    "attention_levels = [0]\n",
    "channel_multipliers = [1]\n",
    "n_heads = 2\n",
    "\n",
    "unet_config = {\n",
    "    'in_channels': in_channels,\n",
    "    'out_channels': out_channels,\n",
    "    'channels': channels,\n",
    "    'n_res_blocks': n_res_blocks,\n",
    "    'attention_levels': attention_levels,\n",
    "    'channel_multipliers': channel_multipliers,\n",
    "    'n_heads': n_heads,\n",
    "}\n",
    "\n",
    "unet = load_config_unet(unet_config)\n",
    "\n",
    "h, w = 120//f, 120//f\n",
    "\n",
    "\n",
    "z = torch.randn(128, 1, h, w)\n",
    "print(\"Shape of input U-NET: \", z.shape)\n",
    "empty_prompt_embed = None\n",
    "print(\"Shape of output: \", unet(z, torch.tensor([10]), empty_prompt_embed).shape)\n",
    "print(\"Number of trainable params: \", sum(p.numel() for p in unet.parameters() if p.requires_grad))\n",
    "\n",
    "latent_scaling_factor = 1\n",
    "n_steps = 1000\n",
    "linear_start =  0.00085\n",
    "linear_end = 0.0012\n",
    "\n",
    "\n",
    "latent_diffusion = MyLatentDiffusion(unet, None, latent_scaling_factor, n_steps, linear_start, linear_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 1000\n",
    "linear_start =  0.00085\n",
    "linear_end = 0.012\n",
    "latent_scaling_factor = 1\n",
    "lr = 5*1e-6\n",
    "BATCH_SIZE = 128\n",
    "MAX_EPOCHS = 50\n",
    "\n",
    "vae = vae.to('cuda')\n",
    "latent_diffusion = MyLatentDiffusion(unet, None, latent_scaling_factor, n_steps, linear_start, linear_end, channels = 1)\n",
    "model = LitLDM(ldm = latent_diffusion, vae = vae, lr = lr, channels = 1).to('cuda')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, num_workers = 12)\n",
    "val_loader = torch.utils.data.DataLoader(eval_data, batch_size=800, num_workers = 12)\n",
    "\n",
    "trainer = L.Trainer(max_epochs=MAX_EPOCHS,\n",
    "                     default_root_dir='ldm', accelerator = 'gpu', \n",
    "                     check_val_every_n_epoch=5)\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "#model = LitLDM.load_from_checkpoint('ldm/lightning_logs/version_3/checkpoints/epoch=149-step=4800.ckpt', ldm = latent_diffusion, vae = vae, latent_dim = latent_dim).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = vae.to('cuda')\n",
    "model = model.to('cuda')\n",
    "trainer = L.Trainer()\n",
    "trainer.validate(model, val_loader)\n",
    "trainer.validate(model, train_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
