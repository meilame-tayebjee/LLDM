{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Sep 15 22:04:23 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.107.02             Driver Version: 550.107.02     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A2000 12GB          Off |   00000000:01:00.0 Off |                  Off |\n",
      "| 30%   37C    P8             10W /   70W |     114MiB /  12282MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      1901      G   /usr/libexec/Xorg                              94MiB |\n",
      "|    0   N/A  N/A      2037      G   /usr/bin/gnome-shell                           13MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "sys.path.append('../../lib/src/')\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from diffusion.stable_diffusion.latent_diffusion import MyLatentDiffusion, LitLDM\n",
    "from diffusion.stable_diffusion.model.unet import UNetModel\n",
    "from diffusion.stable_diffusion.sampler.ddim import DDIMSampler\n",
    "\n",
    "from lib.src.pythae.models import VAE\n",
    "from lib.src.pythae.models.vae import VAEConfig\n",
    "from lib.src.pythae.models import LLDM_IAF, LVAE_IAF_Config, LVAE_IAF\n",
    "from lib.src.pythae.trainers import BaseTrainerConfig, BaseTrainer\n",
    "from lib.scripts.utils import Encoder_ADNI, Decoder_ADNI, My_MaskedDataset\n",
    "from lib.src.pythae.trainers.training_callbacks import WandbCallback\n",
    "\n",
    "from geometric_perspective_on_vaes.sampling import hmc_sampling\n",
    "\n",
    "\n",
    "def load_config_unet(config):\n",
    "    return UNetModel(\n",
    "        in_channels=config['in_channels'],\n",
    "        out_channels=config['out_channels'],\n",
    "        channels=config['channels'],\n",
    "        n_res_blocks=config['n_res_blocks'],\n",
    "        attention_levels=config['attention_levels'],\n",
    "        channel_multipliers=config['channel_multipliers'],\n",
    "        n_heads=config['n_heads'],\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8000, 8, 120])\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "train_data = torch.load('ADNI_train.pt') #(N, T, D)\n",
    "eval_data = torch.load('ADNI_eval.pt')\n",
    "test_data = torch.load('ADNI_test.pt')\n",
    "print(train_data.shape)\n",
    "\n",
    "train_seq_mask = torch.load('ADNI_train_seq_mask.pt') #(N, T)\n",
    "eval_seq_mask = torch.load('ADNI_eval_seq_mask.pt')\n",
    "test_seq_mask = torch.load('ADNI_test_seq_mask.pt')\n",
    "\n",
    "\n",
    "train_pix_mask = torch.ones_like(train_data, requires_grad=False).type(torch.bool)\n",
    "eval_pix_mask = torch.ones_like(eval_data, requires_grad=False).type(torch.bool)\n",
    "test_pix_mask = torch.ones_like(test_data, requires_grad=False).type(torch.bool)\n",
    "\n",
    "train_dataset = My_MaskedDataset(train_data, train_seq_mask, train_pix_mask)\n",
    "eval_dataset = My_MaskedDataset(eval_data, eval_seq_mask, eval_pix_mask)\n",
    "test_dataset = My_MaskedDataset(test_data, test_seq_mask, test_pix_mask)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=200, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(eval_data, batch_size=200, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = Encoder_Chairs(config)\n",
    "# decoder = Decoder_Chairs(config)\n",
    "# vae = LVAE_IAF(config, encoder, decoder)\n",
    "\n",
    "\n",
    "PATH_VAE_FOLDER = 'data-models/adni/trained_models/pre-trained_vae/final_model'\n",
    "PATH_DIFFUSION_CKPT = 'data-models/adni/trained_models/pre-trained_ldm/checkpoints/epoch=49-step=1250.ckpt'\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "vae = VAE.load_from_folder(PATH_VAE_FOLDER).to(device)\n",
    "vae.eval()\n",
    "_, _, _ = vae.retrieveG(train_data[train_seq_mask == 1], verbose = True, T_multiplier=5, device = device, addStdNorm=True)\n",
    "\n",
    "\n",
    "# in_channels = 3\n",
    "# out_channels = 3\n",
    "# channels = 32\n",
    "# n_res_blocks = 2\n",
    "# attention_levels = [2]\n",
    "# channel_multipliers = (1, 2, 4)\n",
    "# n_heads = 16\n",
    "\n",
    "\n",
    "in_channels = 1\n",
    "out_channels = 1\n",
    "channels = 32\n",
    "n_res_blocks = 2\n",
    "attention_levels = [0]\n",
    "channel_multipliers = [1]\n",
    "n_heads = 2\n",
    "\n",
    "unet_config = {\n",
    "    'in_channels': in_channels,\n",
    "    'out_channels': out_channels,\n",
    "    'channels': channels,\n",
    "    'n_res_blocks': n_res_blocks,\n",
    "    'attention_levels': attention_levels,\n",
    "    'channel_multipliers': channel_multipliers,\n",
    "    'n_heads': n_heads,\n",
    "}\n",
    "\n",
    "unet = load_config_unet(unet_config)\n",
    "\n",
    "latent_scaling_factor = 1\n",
    "n_steps = 1000\n",
    "\n",
    "#Pas oublier de modif\n",
    "linear_start =  0.00085\n",
    "linear_end = 0.012\n",
    "\n",
    "input_dim = (1, 120)\n",
    "f = 40 #subsampling factor\n",
    "latent_dim = 1* (120 // f) * (120 // f)\n",
    "print('Latent dim:', latent_dim)\n",
    "\n",
    "\n",
    "latent_diffusion = MyLatentDiffusion(unet, latent_scaling_factor, latent_dim, n_steps, linear_start, linear_end, channels = 1)\n",
    "print('Number of parameters in the diffusion model: ', sum(p.numel() for p in latent_diffusion.parameters() if p.requires_grad))\n",
    "\n",
    "model = LitLDM.load_from_checkpoint(PATH_DIFFUSION_CKPT, ldm = latent_diffusion, vae = vae, latent_dim = latent_dim, lr = 6e-4, channels = 1).to(device)\n",
    "diffusion = model.ldm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diffusion time steps  [997 831 665 499 333 167  83   1]\n",
      "Running on  cuda:0\n",
      "Freezing pre-trained VAE and pre-trained LDM...\n",
      "Freezing done.\n",
      "Number of trainable parameters: 2.0e+04\n",
      "Number of total parameters: 4.0e+05\n"
     ]
    }
   ],
   "source": [
    "model_config = LVAE_IAF_Config(\n",
    "    input_dim=(1, 120),\n",
    "    n_obs_per_ind=train_data.shape[1], #8 for Sprites, 7 as we remove last obs\n",
    "    latent_dim=latent_dim,\n",
    "    beta=0.2,\n",
    "    n_hidden_in_made=2,\n",
    "    n_made_blocks=4,\n",
    "    warmup=5,\n",
    "    context_dim=None,\n",
    "    prior='standard',\n",
    "    posterior='gaussian',\n",
    "    linear_scheduling_steps=10,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "encoder = Encoder_ADNI(model_config.input_dim, model_config.latent_dim).to(device)\n",
    "decoder = Decoder_ADNI(model_config.input_dim, model_config.latent_dim).to(device)\n",
    "ddim_sampler = DDIMSampler(diffusion, time_steps=np.flip([997, 831, 665, 499, 333, 167, 83, 1]).copy(), ddim_eta = 0.25)\n",
    "temperature = 0.25\n",
    "\n",
    "\n",
    "#############\n",
    "lldm = LLDM_IAF(model_config=model_config, encoder=encoder, decoder=decoder, \n",
    "                pretrained_vae=vae, pretrained_ldm=diffusion, ddim_sampler=ddim_sampler,\n",
    "                verbose = True, temp = temperature)\n",
    "\n",
    "lvae = LVAE_IAF(model_config, encoder, decoder).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvae = lvae.train()\n",
    "lvae = lvae.to('cuda')\n",
    "\n",
    "optimizer = torch.optim.Adam(lvae.parameters(), lr=1e-4)\n",
    "\n",
    "training_config = BaseTrainerConfig(\n",
    "        num_epochs=200,\n",
    "        learning_rate=1e-4,\n",
    "        batch_size=200,\n",
    "        steps_saving=50,\n",
    "        steps_predict=100,\n",
    "        shuffle=False,\n",
    "        output_dir='lldm'\n",
    "    )\n",
    "\n",
    "\n",
    "### Scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=4, verbose=True)\n",
    "trainer = BaseTrainer(\n",
    "            model=lvae,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            training_config=training_config,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            #callbacks=callbacks\n",
    "        )\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = [4*1e-4, 2*1e-4, 1e-4, 5*1e-5]\n",
    "\n",
    "for LR in lrs:\n",
    "\n",
    "    optimizer = torch.optim.Adam(lldm.parameters(), lr=LR)\n",
    "\n",
    "    ### Scheduler\n",
    "    training_config = BaseTrainerConfig(\n",
    "            num_epochs=50,\n",
    "            learning_rate=LR,\n",
    "            batch_size=200,\n",
    "            steps_saving=50,\n",
    "            steps_predict=100,\n",
    "            shuffle=False,\n",
    "            output_dir='lldm'\n",
    "        )\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=4, verbose=True)\n",
    "    trainer = BaseTrainer(\n",
    "                model=lldm,\n",
    "                train_dataset=train_dataset,\n",
    "                eval_dataset=eval_dataset,\n",
    "                training_config=training_config,\n",
    "                optimizer=optimizer,\n",
    "                scheduler=scheduler,\n",
    "                #callbacks=callbacks\n",
    "            )\n",
    "    trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
