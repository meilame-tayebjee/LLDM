{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug 12 13:49:45 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.107.02             Driver Version: 550.107.02     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A2000 12GB          Off |   00000000:01:00.0 Off |                  Off |\n",
      "| 30%   39C    P8             10W /   70W |     119MiB /  12282MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      1805      G   /usr/libexec/Xorg                              94MiB |\n",
      "|    0   N/A  N/A      1834      G   /usr/bin/gnome-shell                           18MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#%cd variational_inference_for_longitudinal_data/\n",
    "sys.path.append('../..')\n",
    "sys.path.append('../../lib/src/')\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from diffusion.stable_diffusion.latent_diffusion import MyLatentDiffusion, LitLDM\n",
    "from diffusion.stable_diffusion.model.unet import UNetModel\n",
    "from diffusion.stable_diffusion.sampler.ddim import DDIMSampler\n",
    "\n",
    "from lib.src.pythae.models import VAE\n",
    "from lib.src.pythae.models.vae import VAEConfig\n",
    "from lib.src.pythae.models import LLDM_IAF, LVAE_IAF_Config, LVAE_IAF\n",
    "from lib.src.pythae.trainers import BaseTrainerConfig, BaseTrainer\n",
    "from lib.scripts.utils import Encoder_Chairs,Decoder_Chairs, My_MaskedDataset, make_batched_masks\n",
    "from lib.src.pythae.trainers.training_callbacks import WandbCallback\n",
    "\n",
    "from geometric_perspective_on_vaes.sampling import hmc_sampling\n",
    "\n",
    "\n",
    "def load_config_unet(config):\n",
    "    return UNetModel(\n",
    "        in_channels=config['in_channels'],\n",
    "        out_channels=config['out_channels'],\n",
    "        channels=config['channels'],\n",
    "        n_res_blocks=config['n_res_blocks'],\n",
    "        attention_levels=config['attention_levels'],\n",
    "        channel_multipliers=config['channel_multipliers'],\n",
    "        n_heads=config['n_heads'],\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_sequence(X):\n",
    "\n",
    "    if len(X.shape) == 4:\n",
    "        X = X.unsqueeze(0)\n",
    "\n",
    "    X = X.cpu().detach().numpy()\n",
    "    num_seq = X.shape[0]\n",
    "    num_obs = X.shape[1]\n",
    "\n",
    "    logger = logging.getLogger()\n",
    "    old_level = logger.level\n",
    "    logger.setLevel(100)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(num_seq):\n",
    "        for j in range(num_obs):\n",
    "            plt.subplot(num_seq, num_obs, i*num_obs+j+1)\n",
    "            plt.imshow(X[i, j].transpose(1, 2, 0))\n",
    "            plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "    logger.setLevel(old_level)\n",
    "\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8000, 7, 64, 64, 3])\n",
      "torch.Size([8000, 7, 3, 64, 64])\n",
      "torch.Size([1000, 7, 3, 64, 64])\n",
      "torch.Size([2664, 8, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "PATH_DATA = 'data-models/sprites/data/data/Sprites_train.pt'\n",
    "PATH_DATA_TEST = 'data-models/sprites/data/data/Sprites_test.pt'\n",
    "\n",
    "\n",
    "\n",
    "train_data = torch.load(os.path.join(PATH_DATA))[:-1000, :-1, :, :, :]\n",
    "eval_data = torch.load(os.path.join(PATH_DATA), map_location=\"cpu\")[-1000:, :-1, :, :, :]\n",
    "test_data = torch.load(os.path.join(PATH_DATA_TEST), map_location=\"cpu\")\n",
    "\n",
    "print(train_data.shape)\n",
    "train_data = train_data.permute(0, 1, 4, 2, 3)\n",
    "eval_data = eval_data.permute(0, 1, 4, 2, 3)\n",
    "test_data = test_data.permute(0, 1, 4, 2, 3)\n",
    "print(train_data.shape)\n",
    "print(eval_data.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "train_seq_mask = torch.ones(train_data.shape[:2], requires_grad=False).type(torch.bool)\n",
    "eval_seq_mask = torch.ones(eval_data.shape[:2], requires_grad=False).type(torch.bool)\n",
    "#test_seq_mask = torch.ones(test_data.shape[:2], requires_grad=False).type(torch.bool)\n",
    "train_pix_mask = torch.ones_like(train_data, requires_grad=False).type(torch.bool)\n",
    "eval_pix_mask = torch.ones_like(eval_data, requires_grad=False).type(torch.bool)\n",
    "#test_pix_mask = torch.ones_like(test_data, requires_grad=False).type(torch.bool)\n",
    "\n",
    "train_dataset = My_MaskedDataset(train_data, train_seq_mask, train_pix_mask)\n",
    "eval_dataset = My_MaskedDataset(eval_data, eval_seq_mask, eval_pix_mask)\n",
    "#test_dataset = My_MaskedDataset(test_data, test_seq_mask, test_pix_mask)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=200, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(eval_data, batch_size=200, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_VAE_FOLDER = 'data-models/sprites/trained_models/pre_trained_vae/final_model'\n",
    "PATH_DIFFUSION_CKPT = 'data-models/sprites/trained_models/pre_trained_ldm/checkpoints/epoch=99-step=3200.ckpt'\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "vae = VAE.load_from_folder(PATH_VAE_FOLDER).to(device)\n",
    "vae.eval()\n",
    "_, _, _ = vae.retrieveG(train_data, verbose = True, T_multiplier=5, device = device, addStdNorm=True)\n",
    "\n",
    "in_channels = 3\n",
    "out_channels = 3\n",
    "channels = 64\n",
    "n_res_blocks = 4\n",
    "attention_levels = [0]\n",
    "channel_multipliers = [1]\n",
    "n_heads = 4\n",
    "\n",
    "unet_config = {\n",
    "    'in_channels': in_channels,\n",
    "    'out_channels': out_channels,\n",
    "    'channels': channels,\n",
    "    'n_res_blocks': n_res_blocks,\n",
    "    'attention_levels': attention_levels,\n",
    "    'channel_multipliers': channel_multipliers,\n",
    "    'n_heads': n_heads,\n",
    "}\n",
    "\n",
    "unet = load_config_unet(unet_config)\n",
    "\n",
    "latent_scaling_factor = 1\n",
    "n_steps = 1000\n",
    "\n",
    "#Pas oublier de modif\n",
    "linear_start =  0.00085\n",
    "linear_end = 0.012\n",
    "\n",
    "input_dim = (3, 64, 64)\n",
    "f = 32 #subsampling factor\n",
    "latent_dim = 3* (64 // f) * (64 // f)\n",
    "print('Latent dim:', latent_dim)\n",
    "\n",
    "\n",
    "latent_diffusion = MyLatentDiffusion(unet, latent_scaling_factor, latent_dim, n_steps, linear_start, linear_end)\n",
    "print('Number of parameters in the diffusion model: ', sum(p.numel() for p in latent_diffusion.parameters() if p.requires_grad))\n",
    "\n",
    "model = LitLDM.load_from_checkpoint(PATH_DIFFUSION_CKPT, ldm = latent_diffusion, vae = vae, latent_dim = latent_dim, lr = 6e-4).to(device)\n",
    "diffusion = model.ldm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diffusion time steps  [997 831 665 499 333 167   1]\n",
      "Running on  cuda:0\n",
      "Freezing pre-trained VAE and pre-trained LDM...\n",
      "Freezing done.\n",
      "Number of trainable parameters: 1.1e+06\n",
      "Number of total parameters: 4.4e+06\n"
     ]
    }
   ],
   "source": [
    "model_config = LVAE_IAF_Config(\n",
    "    input_dim=(3, 64, 64),\n",
    "    n_obs_per_ind=train_data.shape[1], #8 for Sprites, 7 as we remove last obs\n",
    "    latent_dim=latent_dim,\n",
    "    beta=0.2,\n",
    "    n_hidden_in_made=2,\n",
    "    n_made_blocks=4,\n",
    "    warmup=0,\n",
    "    context_dim=None,\n",
    "    prior='standard',\n",
    "    posterior='gaussian',\n",
    "    linear_scheduling_steps=10,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "encoder = Encoder_Chairs(model_config).to(device)\n",
    "decoder = Decoder_Chairs(model_config).to(device)\n",
    "ddim_sampler = DDIMSampler(diffusion, n_steps = train_data.shape[1]-1, ddim_eta = 1)\n",
    "temperature = 1\n",
    "\n",
    "\n",
    "#############\n",
    "\n",
    "lldm = LLDM_IAF(model_config=model_config, encoder=encoder, decoder=decoder, \n",
    "                pretrained_vae=vae, pretrained_ldm=diffusion, ddim_sampler=ddim_sampler,\n",
    "                verbose = True, temp = temperature)\n",
    "\n",
    "lvae = LVAE_IAF(model_config, encoder, decoder).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [] # the TrainingPipeline expects a list of callbacks\n",
    "wandb_cb = WandbCallback() # Build the callback \n",
    "# SetUp the callback \n",
    "wandb_cb.setup(\n",
    "    training_config=training_config, # pass the training config\n",
    "    model_config = model_config,\n",
    "    project_name=\"LLDM_reborn\", # specify your wandb project # specify your wandb entity\n",
    ")\n",
    "callbacks.append(wandb_cb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvae = lvae.train()\n",
    "lvae = lvae.to('cuda')\n",
    "\n",
    "optimizer = torch.optim.Adam(lvae.parameters(), lr=1e-3)\n",
    "\n",
    "training_config = BaseTrainerConfig(\n",
    "        num_epochs=200,\n",
    "        learning_rate=1e-4,\n",
    "        batch_size=256,\n",
    "        steps_saving=50,\n",
    "        steps_predict=100,\n",
    "        shuffle=True,\n",
    "        output_dir='lldm'\n",
    "    )\n",
    "\n",
    "\n",
    "### Scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=4, verbose=True)\n",
    "trainer = BaseTrainer(\n",
    "            model=lvae,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            training_config=training_config,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            #callbacks=callbacks\n",
    "        )\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(lldm.parameters(), lr=1e-3)\n",
    "\n",
    "### Scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=4, verbose=True)\n",
    "trainer = BaseTrainer(\n",
    "            model=lldm,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            training_config=training_config,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            #callbacks=callbacks\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
